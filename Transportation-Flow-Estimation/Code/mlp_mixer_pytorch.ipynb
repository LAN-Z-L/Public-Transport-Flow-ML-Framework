{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a12c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch.nn import Conv2d\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9384ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1054df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,dim,hidden_dim,dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            #由此可以看出 FeedForward 的输入和输出维度是一致的\n",
    "            nn.Linear(dim,hidden_dim),\n",
    "            #激活函数\n",
    "            nn.GELU(),\n",
    "            #防止过拟合\n",
    "            nn.Dropout(dropout),\n",
    "            #重复上述过程\n",
    "            nn.Linear(hidden_dim,dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56e3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerBlock(nn.Module):\n",
    "    def __init__(self,dim,token_dim,channel_dim,dropout=0.):\n",
    "        super().__init__()\n",
    "        self.token_mixer=nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Rearrange('b h w -> b w h'),\n",
    "            FeedForward(dim,token_dim,dropout),\n",
    "            Rearrange('b w h -> b h w')\n",
    " \n",
    "         )\n",
    "        self.channel_mixer=nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            FeedForward(dim,channel_dim,dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = x+self.token_mixer(x)\n",
    "        \n",
    "        x = x+self.channel_mixer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e35e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMixer(nn.Module):\n",
    "    def __init__(self,in_channels,dim,num_classes,image_size,depth,token_dim,channel_dim,dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.to_input_arrange = nn.Sequential(Rearrange('b c h w -> b h (c w)'))\n",
    "        # w as the channels -> input size (N,48,48)\n",
    " \n",
    "        # 输入为48*48的table\n",
    "        # 以下为token-mixing MLPs（MLP1）和channel-mixing MLPs（MLP2）各一层\n",
    "        self.mixer_blocks=nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.mixer_blocks.append(MixerBlock(dim,token_dim,channel_dim,dropout))\n",
    " \n",
    "        #\n",
    "        self.layer_normal=nn.LayerNorm(dim)\n",
    " \n",
    "        #\n",
    "        self.mlp_head=nn.Sequential(\n",
    "            #nn.Linear(dim,num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        print('x.shape:', x.shape)\n",
    "        x = self.to_input_arrange(x)\n",
    "        print('input_arrange.shape:', x.shape)\n",
    "        for mixer_block in self.mixer_blocks:\n",
    "            x = mixer_block(x)\n",
    "        x = self.layer_normal(x)    \n",
    "        print('x_combine.shape:', x.shape)\n",
    "        \n",
    "        #x = x.mean(dim=1)\n",
    " \n",
    "        x = self.mlp_head(x)\n",
    "    \n",
    "        '''\n",
    "        for a in x:\n",
    "            size = a.shape[0]\n",
    "            size = int(np.sqrt(size))\n",
    "            for i in range(size):\n",
    "                x[:, i + i * size] = 0\n",
    "        '''\n",
    "                \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3bf78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0646e-04,  9.5762e-01,  2.7629e-01],\n",
      "        [-7.7432e-01, -3.2898e-01, -1.3674e-01]])\n",
      "tensor([[ 0.0324, -0.2371, -0.3519],\n",
      "        [ 0.0359, -0.7703,  0.3345]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.3528, -0.3201, -1.0327],\n",
      "        [ 0.3626, -1.3651,  1.0025]], grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "w1 = torch.nn.Linear(3,3)\n",
    "b= w1(a)\n",
    "print(b)\n",
    "w2 = nn.LayerNorm(3)\n",
    "c = w2(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe646741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([2, 1, 48, 48])\n",
      "input_arrange.shape: torch.Size([2, 48, 48])\n",
      "x_combine.shape: torch.Size([2, 48, 48])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1               [-1, 48, 48]               0\n",
      "         LayerNorm-2               [-1, 48, 48]              96\n",
      "         Rearrange-3               [-1, 48, 48]               0\n",
      "            Linear-4               [-1, 48, 48]           2,352\n",
      "              GELU-5               [-1, 48, 48]               0\n",
      "           Dropout-6               [-1, 48, 48]               0\n",
      "            Linear-7               [-1, 48, 48]           2,352\n",
      "           Dropout-8               [-1, 48, 48]               0\n",
      "       FeedForward-9               [-1, 48, 48]               0\n",
      "        Rearrange-10               [-1, 48, 48]               0\n",
      "        LayerNorm-11               [-1, 48, 48]              96\n",
      "           Linear-12               [-1, 48, 48]           2,352\n",
      "             GELU-13               [-1, 48, 48]               0\n",
      "          Dropout-14               [-1, 48, 48]               0\n",
      "           Linear-15               [-1, 48, 48]           2,352\n",
      "          Dropout-16               [-1, 48, 48]               0\n",
      "      FeedForward-17               [-1, 48, 48]               0\n",
      "       MixerBlock-18               [-1, 48, 48]               0\n",
      "        LayerNorm-19               [-1, 48, 48]              96\n",
      "           Linear-20                 [-1, 2304]         112,896\n",
      "             ReLU-21                 [-1, 2304]               0\n",
      "================================================================\n",
      "Total params: 122,592\n",
      "Trainable params: 122,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.37\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 0.85\n",
      "----------------------------------------------------------------\n",
      "torch.Size([1, 1, 48, 48])\n",
      "x.shape: torch.Size([1, 1, 48, 48])\n",
      "input_arrange.shape: torch.Size([1, 48, 48])\n",
      "x_combine.shape: torch.Size([1, 48, 48])\n",
      "x.shape: torch.Size([1, 1, 48, 48])\n",
      "input_arrange.shape: torch.Size([1, 48, 48])\n",
      "x_combine.shape: torch.Size([1, 48, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "C:\\Users\\remote\\AppData\\Local\\Temp/ipykernel_24200/1165703546.py:36: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for a in x:\n",
      "C:\\Users\\remote\\AppData\\Local\\Temp/ipykernel_24200/1165703546.py:38: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  size = int(np.sqrt(size))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 1, 48, 48])\n",
      "input_arrange.shape: torch.Size([1, 48, 48])\n",
      "x_combine.shape: torch.Size([1, 48, 48])\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MLPMixer(in_channels=1, dim=48, num_classes=48*48, image_size=48, depth=1, token_dim=48,\n",
    "                     channel_dim=48).to(device)\n",
    "    summary(model,(1,48,48))\n",
    " \n",
    "    # torch.Tensor([1, 2, 3, 4, 5, 6])\n",
    "    inputs = torch.Tensor(1, 1, 48, 48)\n",
    "    inputs = inputs.to(device)\n",
    "    print(inputs.shape)\n",
    " \n",
    "    # 将model保存为graph\n",
    "    with SummaryWriter(log_dir='logs', comment='model') as w:\n",
    "        w.add_graph(model, (inputs,))\n",
    "        print(\"success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
